# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z81azUImwAxDXfEsBbFTDM09dC2gzqtQ
"""

import streamlit as st
import pandas as pd
import numpy as np
import xgboost as xgb
import os
import joblib # For saving/loading scikit-learn objects like OneHotEncoder

# --- Configuration & Paths ---
# IMPORTANT: Ensure these paths point to where your saved model and encoder are.
# In a Colab environment, you'd typically mount Google Drive first:
# from google.colab import drive
# drive.mount('/content/drive')
# MODEL_DIR = '/content/drive/My Drive/Colab Notebooks/models' # Example path in Drive
# Replace with your actual path if not using Colab/Drive
MODEL_DIR = 'models' # Assuming 'models' folder is in the same directory as this script
MODEL_NAME = 'xgboost_bond_group_model_v1.json'
ENCODER_NAME = 'onehot_encoder.joblib'

# Full paths
MODEL_PATH = os.path.join(MODEL_DIR, MODEL_NAME)
ENCODER_PATH = os.path.join(MODEL_DIR, ENCODER_NAME)

# --- Helper Function: Feature Engineering (Same as used during training) ---
def apply_feature_engineering_for_prediction(df_to_engineer):
    """
    Applies the same feature engineering steps used during model training.
    This function should match the `apply_feature_engineering` used in your training script.
    """
    df_copy = df_to_engineer.copy()

    # Combinations & Ratios
    # Ensure numerical stability with a small epsilon for division
    df_copy['Income_to_Min_Bond_Investment_Ratio'] = df_copy['Income'] / (df_copy['Bond_Min_Investment'] + 1e-6)
    df_copy['Total_Holdings'] = df_copy['Existing_Equity_Holdings'] + df_copy['Existing_Bond_Holdings']
    df_copy['Holdings_to_Income_Ratio'] = df_copy['Total_Holdings'] / (df_copy['Income'] + 1e-6)
    df_copy['AUM_per_Product'] = df_copy['Total_AUM'] / (df_copy['Num_Other_Products'] + 1)

    # Log Transformations (handle potential zeros with np.log1p)
    df_copy['Log_Income'] = np.log1p(df_copy['Income'])
    df_copy['Log_Total_AUM'] = np.log1p(df_copy['Total_AUM'])
    df_copy['Log_Existing_Equity_Holdings'] = np.log1p(df_copy['Existing_Equity_Holdings'])
    df_copy['Log_Existing_Bond_Holdings'] = np.log1p(df_copy['Existing_Bond_Holdings'])

    # Binning Age (ensure bins and labels match training)
    # Using pd.cut with `include_lowest=True` to include the lower bound of the first bin
    # and `right=False` (left-inclusive) for consistency with previous definition, but np.inf might need adjustment
    # Ensure these bins are exactly as used in training
    df_copy['Age_Bin'] = pd.cut(df_copy['Age'],
                                bins=[20, 35, 45, 55, 65, 100], # Changed np.inf to 100 for explicit upper limit
                                labels=['Young', 'Mid-Career', 'Established', 'Pre-Retirement', 'Senior'],
                                right=False, include_lowest=True)

    # Time-based features
    df_copy['Investment_Frequency_Score'] = 1 / (df_copy['Last_Investment_Days_Ago'] + 1)

    # Return only the relevant features for prediction, drop the target if it was there
    return df_copy


# --- Load Model and Encoder ---
@st.cache_resource # Cache the model and encoder loading to avoid reloading on every rerun
def load_model_and_encoder():
    try:
        # Load the trained XGBoost model
        model = xgb.XGBClassifier() # Initialize the model class first
        model.load_model(MODEL_PATH)
        st.success(f"Successfully loaded XGBoost model from {MODEL_PATH}")

        # Load the fitted OneHotEncoder
        encoder = joblib.load(ENCODER_PATH)
        st.success(f"Successfully loaded OneHotEncoder from {ENCODER_PATH}")

        # IMPORTANT: Retrieve the feature names the model was trained on
        # This is critical for ensuring the input DataFrame for prediction has the exact same columns and order.
        # It's best practice to save these feature names during training.
        # For this setup, we're assuming the model itself might store them, or we have a hardcoded list.
        if hasattr(model, 'feature_names_in_') and model.feature_names_in_ is not None:
             trained_feature_names = list(model.feature_names_in_)
        else:
            # Fallback if model doesn't expose feature_names_in_ directly.
            # This list MUST exactly match the columns of your X_train after OHE during training.
            # If your training feature engineering or OHE changes, this list must be updated.
            st.warning("`model.feature_names_in_` not found or empty. Using a hardcoded feature list.")
            st.warning("PLEASE ENSURE this hardcoded list EXACTLY MATCHES your training features after One-Hot Encoding.")
            trained_feature_names = [
                'Age', 'Income', 'Existing_Equity_Holdings', 'Existing_Bond_Holdings', 'Total_AUM',
                'Num_Other_Products', 'Last_Investment_Days_Ago', 'Bond_Coupon_Rate',
                'Bond_Maturity_Years', 'Bond_Min_Investment', 'Debt_to_Income_Ratio',
                'Portfolio_Fixed_Income_Percent', 'Income_to_Min_Bond_Investment_Ratio',
                'Total_Holdings', 'Holdings_to_Income_Ratio', 'AUM_per_Product',
                'Log_Income', 'Log_Total_AUM', 'Log_Existing_Equity_Holdings',
                'Log_Existing_Bond_Holdings', 'Investment_Frequency_Score',
                # One-hot encoded features - these must be exactly as generated by your encoder
                'Education_Bachelors', 'Education_High School', 'Education_Masters', 'Education_PhD',
                'Risk_Tolerance_Aggressive', 'Risk_Tolerance_Conservative', 'Risk_Tolerance_Moderate',
                'Online_Platform_Usage_Freq_High', 'Online_Platform_Usage_Freq_Low', 'Online_Platform_Usage_Freq_Medium',
                'Bond_Credit_Rating_A', 'Bond_Credit_Rating_AA', 'Bond_Credit_Rating_AAA',
                'Age_Bin_Established', 'Age_Bin_Mid-Career', 'Age_Bin_Pre-Retirement', 'Age_Bin_Senior', 'Age_Bin_Young'
            ]
            # Verify that the feature names list above is up-to-date with your actual training features
            # One way to do this after training: print(X_train.columns.tolist())
            # and paste that output here.


        return model, encoder, trained_feature_names
    except FileNotFoundError:
        st.error(f"Model or encoder files not found in '{MODEL_DIR}'.")
        st.error(f"Please ensure '{MODEL_PATH}' and '{ENCODER_PATH}' exist and are accessible.")
        st.info("Remember to run the training script once and save the model/encoder to the specified directory.")
        st.stop() # Stop the app if files are not found
    except Exception as e:
        st.error(f"Error loading model or encoder: {e}")
        st.stop()


# --- Preprocessing Function for New Data ---
def preprocess_for_prediction(df_raw_input, encoder, trained_feature_names):
    """
    Applies the same preprocessing (One-Hot Encoding) as used during training
    to a new raw input DataFrame. Ensures feature consistency with training data.
    """
    df_processed = df_raw_input.copy()

    # Identify categorical columns as they were in the original data before OHE
    # This list should match the original categorical columns in your training data
    original_categorical_cols = [
        'Education', 'Risk_Tolerance', 'Online_Platform_Usage_Freq', 'Bond_Credit_Rating', 'Age_Bin'
    ]

    # Transform categorical features using the loaded encoder
    # Ensure all original_categorical_cols are present in df_processed before transforming
    for col in original_categorical_cols:
        if col not in df_processed.columns:
            df_processed[col] = pd.NA # Or some appropriate default for missing original categorical

    X_encoded = encoder.transform(df_processed[original_categorical_cols])
    X_encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(original_categorical_cols), index=df_processed.index)

    # Drop original categorical columns and concatenate encoded ones
    X_processed = pd.concat([df_processed.drop(columns=original_categorical_cols), X_encoded_df], axis=1)

    # IMPORTANT: Ensure feature consistency with the training data.
    # Add missing columns (features present in training but not in this new data), filled with zeros
    missing_in_new_data = list(set(trained_feature_names) - set(X_processed.columns))
    for col in missing_in_new_data:
        X_processed[col] = 0.0 # Use 0.0 for numerical consistency

    # Drop extra columns (features present in new data but not in training data)
    extra_in_new_data = list(set(X_processed.columns) - set(trained_feature_names))
    if extra_in_new_data:
        X_processed = X_processed.drop(columns=extra_in_new_data)

    # Ensure the order of columns is identical to training data
    X_processed = X_processed[trained_feature_names]

    return X_processed


# --- Streamlit App Layout ---
st.set_page_config(layout="wide")
st.title("ðŸ’° Bond Purchase Propensity Predictor (Manual Input)")
st.write("Enter customer and bond details to predict purchase likelihood.")

# --- Load Model and Encoder (Cached) ---
model, encoder, trained_feature_names = load_model_and_encoder()

# --- Input Form ---
st.header("Customer and Bond Details")

col1, col2, col3 = st.columns(3)

with col1:
    st.subheader("Customer Profile")
    age = st.slider("Age", 20, 80, 45)
    income = st.number_input("Annual Income (THB)", 10000, 5000000, 500000, step=10000)
    education = st.selectbox("Education Level", ['High School', 'Bachelors', 'Masters', 'PhD'])
    risk_tolerance = st.selectbox("Risk Tolerance", ['Conservative', 'Moderate', 'Aggressive'])

with col2:
    st.subheader("Customer Financials")
    existing_equity_holdings = st.number_input("Existing Equity Holdings (THB)", 0, 10000000, 1000000, step=100000)
    existing_bond_holdings = st.number_input("Existing Bond Holdings (THB)", 0, 20000000, 5000000, step=100000)
    num_other_products = st.slider("Number of Other Products Held", 0, 10, 2)
    last_investment_days_ago = st.slider("Days Since Last Investment", 1, 1000, 90)
    online_platform_usage_freq = st.selectbox("Online Platform Usage Frequency", ['Low', 'Medium', 'High'])

with col3:
    st.subheader("Bond Details (Group 3 Example)")
    bond_coupon_rate = st.slider("Bond Coupon Rate (%)", 0.01, 0.10, 0.035, step=0.001, format="%.3f")
    bond_maturity_years = st.slider("Bond Maturity (Years)", 1, 30, 10)
    bond_credit_rating = st.selectbox("Bond Credit Rating", ['AAA', 'AA', 'A', 'BBB']) # Include BBB as a potential option even if not in trained AAA,AA,A
    bond_min_investment = st.number_input("Bond Minimum Investment (THB)", 10000, 5000000, 500000, step=10000)
    # Total_AUM and Debt_to_Income_Ratio will be calculated, not input directly
    # Portfolio_Fixed_Income_Percent will be calculated, not input directly


if st.button("Predict Purchase Propensity"):
    # Create a single row DataFrame from user inputs
    input_data = pd.DataFrame([{
        'Customer_ID': 'Manual_Input_Customer', # Dummy ID for single input
        'Age': age,
        'Income': income,
        'Education': education,
        'Risk_Tolerance': risk_tolerance,
        'Existing_Equity_Holdings': existing_equity_holdings,
        'Existing_Bond_Holdings': existing_bond_holdings,
        'Total_AUM': income * np.random.uniform(1, 5) + existing_equity_holdings + existing_bond_holdings, # Simplified calculation
        'Num_Other_Products': num_other_products,
        'Last_Investment_Days_Ago': last_investment_days_ago,
        'Online_Platform_Usage_Freq': online_platform_usage_freq,
        'Bond_Coupon_Rate': bond_coupon_rate,
        'Bond_Maturity_Years': bond_maturity_years,
        'Bond_Credit_Rating': bond_credit_rating,
        'Bond_Min_Investment': bond_min_investment,
        'Debt_to_Income_Ratio': np.random.uniform(0.1, 0.8), # Simplified for manual input, or calculate from more inputs
        'Portfolio_Fixed_Income_Percent': existing_bond_holdings / (income * np.random.uniform(1, 5) + existing_equity_holdings + existing_bond_holdings + 1e-6)
    }])

    # Apply feature engineering
    df_fe_input = apply_feature_engineering_for_prediction(input_data)

    # Preprocess the data using the loaded encoder and trained features
    final_input_df = preprocess_for_prediction(df_fe_input, encoder, trained_feature_names)

    # Make prediction
    with st.spinner("Calculating propensity..."):
        prediction_proba = model.predict_proba(final_input_df)[:, 1][0] # Get the probability for the single customer

    st.subheader("Prediction Result")
    if prediction_proba > 0.5: # Example threshold
        st.success(f"**High Potential Buyer!** Predicted Propensity: **{prediction_proba:.2%}**")
    else:
        st.info(f"Predicted Propensity: {prediction_proba:.2%}")
    st.write("*(Note: A higher percentage indicates a higher likelihood of purchasing this bond.)*")

    st.subheader("Input Data for Prediction (after preprocessing)")
    st.dataframe(final_input_df)


st.markdown("---")
st.markdown("Disclaimer: This is a simulated model for demonstration purposes only. Do not use for actual financial decisions.")